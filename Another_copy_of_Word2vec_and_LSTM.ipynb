{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8qqRwYvGJZn2"
   },
   "outputs": [],
   "source": [
    "!pip install torch==2.3.0\n",
    "!pip install torchtext==0.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dnXCMDWz7XR-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import sklearn as sk\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "import torchtext.vocab as tvc\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E7oxskrGvW7e"
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load Training Data\n",
    "data = pd.read_csv(\"sample_data/labeled_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Q4T-VUs9NcN",
    "outputId": "50274770-965b-4539-b120-d1d9eca42a3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "\n",
    "wordnet = WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english')\n",
    "def clean_data(text, stop_words, max_length):\n",
    "  text = text.lower()\n",
    "  text = text.replace(\"'\",\"\")\n",
    "  text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "  text = text.split()\n",
    "  text = [word for word in text if word not in stop_words]\n",
    "  text = [wordnet.lemmatize(word) for word in text]\n",
    "  text = text[:max_length]\n",
    "  return text\n",
    "\n",
    "data[\"tweet\"] = data[\"tweet\"].apply(clean_data, stop_words = stop_words, max_length = 1000)\n",
    "dataset_y = data[\"class\"].tolist()\n",
    "dataset_x = data[\"tweet\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create training sets\n",
    "\n",
    "X_train = dataset_x\n",
    "Y_train = dataset_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3f_ctQO0O1hH",
    "outputId": "c73e07e9-81ab-4bba-f2af-c54bdf8c4449"
   },
   "outputs": [],
   "source": [
    "#Build/Load the Vocabulary\n",
    "\n",
    "min_freq = 2\n",
    "\n",
    "unk_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [unk_token, pad_token]\n",
    "special_tokens_output = []\n",
    "\n",
    "Y_train = np.array(Y_train).reshape(-1,1)\n",
    "if 'vocabulary_hate_speech.pt' == None:\n",
    "    input_train_dataset = tvc.build_vocab_from_iterator(X_train, min_freq = min_freq, specials = special_tokens)\n",
    "    torch.save(input_train_dataset, 'vocabulary_hate_speech.pt')\n",
    "else:\n",
    "    input_train_dataset = torch.load('vocabulary_hate_speech.pt')\n",
    "input_train_dataset.set_default_index(input_train_dataset[unk_token])\n",
    "Y_train = Y_train.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "otA4PoROULzw"
   },
   "outputs": [],
   "source": [
    "#Numerize Inputs and Outputs and Pad Inputs\n",
    "\n",
    "for i in range(len(Y_train)):\n",
    "  X_train[i] = torch.tensor(input_train_dataset.lookup_indices(X_train[i]))\n",
    "X_train = nn.utils.rnn.pad_sequence(X_train, padding_value = 0, batch_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ON7G_FtzVAP9"
   },
   "outputs": [],
   "source": [
    "#Merge X, Y into a dataset and divide into batches with 32 samples in each batch\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, x, y):\n",
    "    self.x = x\n",
    "    self.y = y\n",
    "  def __len__(self):\n",
    "    return len(self.x)\n",
    "  def __getitem__(self, idx):\n",
    "    return self.x[idx], self.y[idx]\n",
    "data = CustomDataset(X_train, Y_train)\n",
    "dataloader = DataLoader(data, batch_size = 32, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iq68t8h4Y3lT"
   },
   "outputs": [],
   "source": [
    "#Build the Encoder Model\n",
    "\n",
    "class Encode(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, output_size, num_layers,embedding_size, dropout):\n",
    "    super(Encode, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_layers = num_layers\n",
    "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, batch_first = True, dropout = dropout)\n",
    "    self.ReLU = nn.ReLU()\n",
    "    self.fc = nn.Linear(hidden_size, output_size)\n",
    "    self.fc1 = nn.Linear(31, 3)\n",
    "\n",
    "  def forward(self, x):\n",
    "    embedding = self.dropout(self.embedding(x))\n",
    "    output, (hidden, cell) = self.lstm(embedding)\n",
    "    output = self.fc(output)\n",
    "    out = self.fc1(output.reshape(-1,31))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v5Y4YY0SfhYO"
   },
   "outputs": [],
   "source": [
    "#Initialize model, loss, and optimizer function\n",
    "\n",
    "input_size = len(input_train_dataset)\n",
    "hidden_size = 100\n",
    "output_size = 1\n",
    "num_layers = 2\n",
    "embedding_size = 128\n",
    "dropout = 0.5\n",
    "\n",
    "model = Encode(input_size, hidden_size, output_size, num_layers, embedding_size, dropout)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVsWCQnX7XjZ"
   },
   "outputs": [],
   "source": [
    "#Load Model Checkpoint\n",
    "\n",
    "check_point = torch.load(\"check-point-hate-speech.pt\")\n",
    "model.load_state_dict(check_point['model_state_dict'])\n",
    "optimizer.load_state_dict(check_point['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LVyDwA4veckQ"
   },
   "outputs": [],
   "source": [
    "#Train model\n",
    "\n",
    "num_epochs = 20\n",
    "valid_loss = 0\n",
    "count = 0\n",
    "for epoch in range(num_epochs):\n",
    "  for input, output in dataloader:\n",
    "    output = output[0]\n",
    "    y_pred = model(input)\n",
    "    l = loss(y_pred, output)\n",
    "    valid_loss += l.item()\n",
    "    count += 1\n",
    "    l.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"Epoch: {epoch}, Loss: {valid_loss/float(count)}\")\n",
    "    if count % 100 == 0:\n",
    "      torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                }, \"check-point.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JhddyqnXuzKg"
   },
   "outputs": [],
   "source": [
    "#Load, Clean, and Divide test file into X_test and Y_test\n",
    "\n",
    "data_test = pd.read_csv(\"sample_data/testhatespeech.csv\")\n",
    "data_test[\"tweet\"] = data_test[\"tweet\"].apply(clean_data, stop_words = stop_words, max_length = 1000)\n",
    "def minimize(text, max_length):\n",
    "  text = text[:max_length]\n",
    "  return text\n",
    "data_test[\"tweet\"] = data_test[\"tweet\"].apply(minimize, max_length = 31)\n",
    "X_test = data_test[\"tweet\"].tolist()\n",
    "Y_test = data_test[\"Toxicity\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Flan_AkuYkn3"
   },
   "outputs": [],
   "source": [
    "#Numberize and Pad\n",
    "\n",
    "Y_test = np.array(Y_test).reshape(-1,1)\n",
    "Y_test = Y_test.tolist()\n",
    "for j in range(len(X_test)):\n",
    "  X_test[j] = torch.tensor(input_train_dataset.lookup_indices(X_test[j]))\n",
    "X_test = nn.utils.rnn.pad_sequence(X_test, padding_value = 0, batch_first = True)\n",
    "Y_test = torch.tensor(Y_test, dtype = torch.float32)\n",
    "Y_test = Y_test.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWdgO-Ji6XjM"
   },
   "outputs": [],
   "source": [
    "#Conduct Model Testing\n",
    "\n",
    "with torch.no_grad():\n",
    "  y_pred = model(X_test)\n",
    "  y_pred_cls = torch.max(y_pred,1)\n",
    "  for i in range (y_pred_cls.indices.shape[0]):\n",
    "    if y_pred_cls.indices[i] == 2:\n",
    "      y_pred_cls.indices[i] = 0\n",
    "  acc = y_pred_cls.indices.eq(Y_test).sum() / float(3555)\n",
    "  print(f\"The accuracy of the model through testing: {acc.item()*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WKxdcs_ZdUGo"
   },
   "outputs": [],
   "source": [
    "#Model Testing by Inputs from Keyboard\n",
    "\n",
    "nlp_input = \"I love this movie guys\"\n",
    "nlp_input = clean_data(nlp_input, stop_words, 1000)\n",
    "input = input_train_dataset.lookup_indices(nlp_input)\n",
    "for m in range(31 - len(nlp_input)):\n",
    "  input.append(0)\n",
    "with torch.no_grad():\n",
    "  input = torch.tensor(input)\n",
    "  input = input.reshape(-1,31)\n",
    "  print(torch.argmax(model(input)))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
